<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>Audio Cognition</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 16px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h1 style="color: #ccc">Intelligent Reasoning System</h1>
<h1 id="audio-cognition">Audio Cognition</h1>
<div class="badge">
    <span class="key">Type</span>
    <span class="value">Course</span>
</div>
<div class="badge">
    <span class="key">Instructor</span>
    <span class="value">Gary Leung</span>
</div>
<div class="badge">
    <span class="key">Institution</span>
    <span class="value">NUS-ISS</span>
</div>
<div class="badge">
    <span class="key">Note Updated</span>
    <span class="value">2025-07-25</span>
</div>
<h2 id="learning-outcomes">Learning Outcomes</h2>
<p>By the end of this course, you will be able to:</p>
<ol>
<li>Explain how machines perceive, understand, and interpret auditory inputs such as <em>sound</em>, <em>speech</em>, and <em>music</em>.</li>
<li>Describe the fundamental concepts of <strong>audio signal processing</strong>.</li>
<li>Evaluate the performance of state-of-the-art models, including <strong>audio foundation models</strong>, in audio cognition tasks.</li>
</ol>
<h2 id="audio-speech-and-music">Audio, Speech, and Music</h2>
<ol>
<li><strong>Audio</strong><br>
Vibrations that travel through a medium and are perceived by the ear. Includes speech, music, sound effects, and ambient noise.</li>
<li><strong>Speech</strong><br>
Vocal communication using vocal cords to produce meaningful sounds. Used for information exchange, emotional expression, and social interaction.</li>
<li><strong>Music</strong><br>
Artistic expression through organised sounds and rhythms.
<ul>
<li><strong>Elements</strong>: melody, harmony, rhythm, instrumentation, vocals, lyrics, arrangement.</li>
<li><strong>Purpose</strong>: aethetic enjoyment and emotional impact.</li>
</ul>
</li>
</ol>
<h2 id="audio-applications">Audio Applications</h2>
<p>Machines can interpret and respond to auditory inputs in various real-world applications, including:</p>
<ol>
<li><strong>Voice Assistants and Smart Speakers</strong>
<ul>
<li>Used for hand-free queries, reminders, smart home control, and more.</li>
<li>Examples: Amazon Alexa, Google Assistant, Apple Siri, Microsoft Cortana.</li>
</ul>
</li>
<li><strong>Interactive Voice Response (IVR) Systems</strong>
<ul>
<li>Deployed in customer service phone lines for banks, airlines, and utility providers.</li>
<li>Allow automated call routing and self-service interactions via voice commands.</li>
</ul>
</li>
<li><strong>Voice Command Systems in Vehicles</strong>
<ul>
<li>Enable drivers to control GPS, music, and calls without taking hands off the wheel.</li>
</ul>
</li>
<li><strong>Voice-Activated Home Automation</strong>
<ul>
<li>Controls lighting, thermostats, appliances, and security systems using spoken commands.</li>
</ul>
</li>
<li><strong>Language Learning Apps</strong>
<ul>
<li>Provide interactive speaking and listening practice for learners.</li>
<li>Offer pronouncation feedback and conversational simulations.</li>
</ul>
</li>
<li><strong>Speech Therapy Tools</strong>
<ul>
<li>Assist individuals with speech impairments through guided exercises and feedback.</li>
<li>Used in clinical and at-home rehabilitation.</li>
</ul>
</li>
</ol>
<h2 id="audio-signal-processing">Audio Signal Processing</h2>
<h3 id="sampling-rate">Sampling Rate</h3>
<p>What is Sampling?</p>
<ol>
<li><strong>Analog audio</strong> is a continuous waveform.</li>
<li><strong>Sampling</strong> converts this analog signal into a digital one by measuring it at regular time intervals.</li>
<li>Each measurement is called a <strong>sample</strong>.</li>
<li>The <strong>sampling rate</strong> is how many samples are taken per second (measured in <strong>Hz</strong>).</li>
</ol>
<blockquote>
<p><img src="file:////Users/naryaong/Code/git/ngpeijiun.github.io/ais/irs/_media/irs-2-sampling-rate.png" alt="Sampling Rate"></p>
</blockquote>
<p>Human Auditory Range</p>
<ol>
<li>The human ear typically hears frequencies from <strong>50 Hz to 15 kHz</strong>.</li>
<li>To capture all these frequencies, a sufficiently high sampling rate is needed.</li>
</ol>
<p>Nyquist Theorem</p>
<ol>
<li>To accurately reconstruct a signal, the sampling rate should be <strong>at least twice the highest frequency</strong> in the signal.</li>
<li>For speech (~ 15 kHz max), at least <strong>30 kHz</strong> is required.</li>
</ol>
<p>Common Sampling Rates</p>
<table>
<thead>
<tr>
<th style="text-align:left">Use Case</th>
<th style="text-align:left">Sampling Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Telephone-grade audio</td>
<td style="text-align:left">8 kHz</td>
</tr>
<tr>
<td style="text-align:left">Mobile/PC voice communication</td>
<td style="text-align:left">16 kHz</td>
</tr>
<tr>
<td style="text-align:left">CD-quality audio</td>
<td style="text-align:left">44.1 kHz</td>
</tr>
</tbody>
</table>
<h3 id="audio-coding">Audio Coding</h3>
<p>Quantisation</p>
<ol>
<li>Continuous signal levels are <strong>quantised to discrete values</strong>.</li>
<li>Each sample is represented as a fixed-point number in computer. Example: 16-bit representation ranges from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>32</mn><mo separator="true">,</mo><mn>767</mn></mrow><annotation encoding="application/x-tex">-32,767</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">−</span><span class="mord">32</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">767</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo separator="true">,</mo><mn>767</mn></mrow><annotation encoding="application/x-tex">32,767</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">32</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">767</span></span></span></span>.</li>
</ol>
<p>Encoding Format</p>
<ol>
<li><strong>Linear Coding</strong>
<ul>
<li><strong>Pulse-Code Modulation (PCM)</strong> is the basic method for digitally representing audio signals.</li>
<li>16-bit PCM is widely used in <strong>speech processing</strong>.</li>
</ul>
</li>
<li><strong>Non-linear Coding</strong>
<ul>
<li><strong>A-law</strong> (Europe) and <strong>μ-law (mu-law)</strong> (US and Japan) use only 256 levels with 8-bit encoding.</li>
<li>Logarithmic encoding (A-law/μ-law) allocates more resolution to quieter sounds, where human hearing is more sensitive. Despite using fewer bits, non-linear coding preserves quality—making it efficient for bandwidth-limited systems like traditional telephony.</li>
</ul>
</li>
</ol>
<h3 id="file-formats">File Formats</h3>
<p>Popular Formats</p>
<ol>
<li><strong>WAV</strong>: PCM-based, uncompressed, lossless. Developed by Microsoft and IBM.</li>
<li><strong>MP3</strong>: Lossy compression format.</li>
<li><strong>FLAC</strong>: Free Lossless Audio Codec, widely supported.</li>
<li><strong>OGG</strong>: Free multimedia container format, supports various codecs.</li>
</ol>
<p>Speech Processing</p>
<ol>
<li>Usually uses <em>non-compressed PCM</em> format for audio input.</li>
<li>Speech recognition models are often tailored for <em>specific sampling rates</em>.</li>
</ol>
<h3 id="spectrogram">Spectrogram</h3>
<p>To better understand how audio signals vary over time and frequency, we use <strong>spectrograms</strong>, which transform 1D waveforms into 2D representations.</p>
<ol>
<li><strong>Waveform</strong> (1D): Represents amplitude over time.</li>
<li><strong>Spectrogram</strong> (2D): Represents how frequencies evolve over time.
<ul>
<li><strong>X-axis</strong> → Time</li>
<li><strong>Y-axis</strong> → Frequency</li>
<li><strong>Colour Intensity</strong> → Amplitude/Energy (e.g., yellow = strong signal, purple = weak signal)</li>
</ul>
</li>
</ol>
<p>The spectrogram shows which frequencies are active at each time slice and how strong they are.</p>
<h2 id="speech-and-audio-systems">Speech and Audio Systems</h2>
<p><strong>Evolution of Speech Recognition Systems</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">Era</th>
<th style="text-align:left">Method</th>
<th style="text-align:left">Characteristics</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Since 1952</td>
<td style="text-align:left">Pattern Matching</td>
<td style="text-align:left"><span style="color: #888">- Early systems for isolated word recognition<br>- Uses templates and distance metrics</span></td>
</tr>
<tr>
<td style="text-align:left">Since 1980</td>
<td style="text-align:left">HMM Models (GMM-HMM)</td>
<td style="text-align:left"><span style="color: #888">- Statistical modelling of speech sequences<br>- Suitable for <strong>continuous speech</strong><br>- <strong>Accuracy limited</strong> by hand-crafted features</span></td>
</tr>
<tr>
<td style="text-align:left">Since 2009</td>
<td style="text-align:left">Deep Learning (DNN-HMM)</td>
<td style="text-align:left"><span style="color: #888">- Replaces GMMs with DNNs for acoustic modelling<br>- Improves robustness on <strong>natural speech</strong><br>- Language models were still largely n-gram-based, not LLMs</span></td>
</tr>
<tr>
<td style="text-align:left">Since 2017</td>
<td style="text-align:left">End-to-End ASR</td>
<td style="text-align:left"><span style="color: #888">- Replaces pipeline with <strong>unified neural architecture</strong> (e.g., encoder-decoder, CTC loss function)<br>- Requires <strong>large speech datasets</strong><br>- Better generalisation and latency</span></td>
</tr>
</tbody>
</table>
<p><strong>Challenges in Automatic Speech Recognition (ASR)</strong></p>
<p>Common challenges affecting ASR accuracy:</p>
<ol>
<li><strong>Device and Channel Mismatch</strong><br>
Different microphones and hardware cause signal variations.</li>
<li><strong>Background Noise</strong><br>
Environmental sounds reduce signal clarity.</li>
<li><strong>Speaker Differences</strong><br>
Variations in pitch, pace, gender, and style.</li>
<li><strong>Similar Pronunciations</strong><br>
Hard to disambiguate based on audio alone. E.g., &quot;write&quot; vs &quot;right&quot;.</li>
<li><strong>Accent and Dialect Variation</strong><br>
Regional and cultural pronunciation differences.</li>
<li><strong>Pronunciation Variation in Continuous Speech</strong><br>
Words blend together; less clear articulation.</li>
<li><strong>Prosody Differences</strong><br>
Intonation, rhythm, and stress patterns impact recognition.</li>
<li><strong>Homographs (Multiple Pronunciations)</strong><br>
Words like &quot;read&quot;:
<ul>
<li>Present: /riːd/ (reed)</li>
<li>Past: /rɛd/ (red)</li>
</ul>
</li>
</ol>
<p>Also consider challenges like reverberation, overlapping speech, and <a href="https://languagelog.ldc.upenn.edu/nll/?p=60956">spontaneous vs. read speech</a>.</p>
<p><strong>Evolution of Text-to-Speech (TTS) Systems</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">Era</th>
<th style="text-align:left">Method</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Since 1970s</td>
<td style="text-align:left">Early Methods</td>
<td style="text-align:left"><span style="color: #888">- Based on <strong>formant synthesis</strong> and <strong>PSOLA</strong><br>- Fully rule-based and robotic sounding</span></td>
</tr>
<tr>
<td style="text-align:left">Since 1990s</td>
<td style="text-align:left">Concatenation Methods</td>
<td style="text-align:left"><span style="color: #888">- <strong>Unit selection</strong> from pre-recorded speech<br>- Higher naturalness, less flexible</span></td>
</tr>
<tr>
<td style="text-align:left">Since 2000</td>
<td style="text-align:left">HHM-Based Methods</td>
<td style="text-align:left"><span style="color: #888">- Models speech parameters using <strong>HMMs</strong><br>- Combines with <strong>vocoder</strong> for waveform generation</span></td>
</tr>
<tr>
<td style="text-align:left">Since 2012</td>
<td style="text-align:left">Deep Learning</td>
<td style="text-align:left"><span style="color: #888">- Uses <strong>DNNs</strong> to model acoustic features<br>- Improves prosody and expressiveness</span></td>
</tr>
<tr>
<td style="text-align:left">Since 2017</td>
<td style="text-align:left">End-to-End Models</td>
<td style="text-align:left"><span style="color: #888">- Fully neural systems (e.g., Tacotron, WaveNet)<br>- Learns mapping from text to waveform directly<br>- High quality, human-like speech</span></td>
</tr>
</tbody>
</table>
<h2 id="paradigm-shift">Paradigm Shift</h2>
<p>A paradigm shift in AI and audio development, transitioning through:</p>
<ol>
<li><strong>Machine Learning</strong>
<ul>
<li>Focus: &quot;<em>how</em>&quot; (methods)</li>
<li>Emphasis: learning algorithms</li>
<li>Trend: homogenisation of learning techniques</li>
</ul>
</li>
<li><strong>Deep Learning</strong>
<ul>
<li>Focus: features</li>
<li>Emphasis: architectures</li>
</ul>
</li>
<li><strong>Foundational Models</strong>
<ul>
<li>Focus: functionalities</li>
<li>Emphasis: models</li>
</ul>
</li>
</ol>
<h2 id="audio-foundation-models">Audio Foundation Models</h2>
<p>Foundation models, trained on large-scale <strong>multimodal</strong> data such as text, images, and speech, are adapted for specific tasks, transforming general-purpose models into specialised systems. With transformer-based architectures, these models have significantly advanced speech recognition by learning rich audio representations. These models scale up to billions of parameters and support tasks such as <strong>automatic speech recognition (ASR)</strong>, <strong>speaker identification</strong>, and <strong>emotion detection</strong>.</p>
<p>Source: Bommasani et al. (2021), <em>On the Opportunities and Risks of Foundation Models</em></p>
<p>Categories of Tasks</p>
<ol>
<li><strong>Discriminative Tasks</strong><br>
Make discrete decisions on continuous speech input (e.g., classification).</li>
<li><strong>Generative Tasks</strong><br>
Generate continuous speech from input sources (e.g., text-to-speech, music generation).</li>
</ol>
<p>Examples</p>
<ol>
<li><strong>Speech Foundation Models</strong>: Whisper, Wav2Vec 2.0</li>
<li><strong>Audio/Music Generative Models</strong>: AudioGen, AudioLDM2, MusicGen</li>
</ol>
<p>Multi-Task Capability</p>
<ol>
<li>Supports different tasks using a single model.</li>
<li>Supports multiple spoken languages.</li>
</ol>
<p>Multimodality</p>
<ol>
<li>Mirrors human perception across senses.</li>
<li>Enables more user-friendly and flexible interfaces.</li>
<li>Facilitates comprehensive problem-solving through multi-modal inputs.</li>
</ol>
<h3 id="whisper-model">Whisper Model</h3>
<ol>
<li>Trained on 680k hours of multilingual and multitask supervised data.</li>
<li>Supports three key tasks:
<ul>
<li>Speech Recognition</li>
<li>Speech Translation</li>
<li>Language Recognition</li>
</ul>
</li>
<li>Model variants with parameter sizes: 39M, 74M, 244M, 769M, 1550M.</li>
</ol>
<p>Architecture</p>
<ol>
<li><strong>Encoder-Decoder</strong> structure with <em>positional encodings</em> and <em>cross-attention</em>.</li>
<li>Input: <strong>Log-Mel Spectrogram</strong>.</li>
<li>Uses multitask token formatting (e.g., SOT, language tag, transcription).</li>
</ol>
<p>Processing Flow</p>
<ol>
<li>Language identification</li>
<li>Trascribe or translate speech</li>
<li>Output:
<ul>
<li>Text-only transcription or</li>
<li>Time-aligned transcript with token timestamps</li>
</ul>
</li>
</ol>
<h3 id="wav2vec-20">Wav2Vec 2.0</h3>
<p><strong>Wav2Vec 2.0</strong> is a state-of-the-art framework developed by Facebook AI for learning speech representations from raw waveforms using <strong>self-supervised learning (SSL)</strong>. It reduces reliance on large labelled datasets and enables effective training for <strong>low-resource languages</strong> (languages with scarce digital/audio data).</p>
<p><strong>Architecture and Training</strong></p>
<p>Wav2Vec 2.0 operates in two stages:</p>
<ol>
<li><strong>Pretraining</strong> (Unsupervised)
<blockquote>
<p><img src="file:////Users/naryaong/Code/git/ngpeijiun.github.io/ais/irs/_media/irs-4-wav2vec.png" alt="Wav2Vec Pretraining" title="Wav2Vec Pretraining"></p>
</blockquote>
<ul>
<li>Uses <em>unlabelled</em> audio to train an <strong>upstream model</strong>.</li>
<li>A <strong>CNN encoder</strong> transforms the raw waveform into <strong>latent speech representations</strong> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span></span></span></span>).</li>
<li>Certain time steps in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span></span></span></span> are randomly <strong>masked</strong>.</li>
<li>A <strong>transformer</strong> processes the full sequence, outputting <strong>contextual representations</strong> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>).</li>
<li>The masked latent vectors are <strong>quantised online</strong> to form discrete targets (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span>).</li>
<li>The model is trained to maximise similarity between <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span> using <strong>contrastive loss</strong>.</li>
</ul>
</li>
<li><strong>Fine-tuning</strong> (Supervised)
<ul>
<li>The pretrained model is either:
<ul>
<li>Frozen and used to extract features, or</li>
<li>Fine-tuned with labelled data for <strong>downstream tasks</strong> such as:
<ul>
<li><strong>ASR</strong> (e.g., speech-to-text)</li>
<li><strong>SID</strong> (speaker identification)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>Strengths</strong></p>
<p>Achieves strong performance even with <em>limited labelled data</em>.</p>
<p>Demonstrated <strong>Word Error Rate (WER)</strong> results:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Labelled Data</th>
<th style="text-align:left">WER (Clean / Other)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">960 hours</td>
<td style="text-align:left">1.8% / 3.3%</td>
</tr>
<tr>
<td style="text-align:left">100 hours</td>
<td style="text-align:left">2.0% / 4.0%</td>
</tr>
<tr>
<td style="text-align:left">10 minutes</td>
<td style="text-align:left">4.8% / 8.2%</td>
</tr>
</tbody>
</table>
<p>This level of performance with just <em>10 minutes of labelled data</em> is typically only achieved with thousands of hours using traditional models.</p>
<p><strong>Summary</strong></p>
<ol>
<li>
<p><strong>Self-supervised learning</strong> enables efficient use of <em>unlabelled data</em>.</p>
</li>
<li>
<p>Supports <strong>multilingual</strong> and <strong>low-resource languages</strong>.</p>
<p><span style="color: #888">After pretraining, only a small amount of labelled data (even as little as 10 minutes) is needed to adapt to a specific language. It learns <strong>universal phonetic features</strong> (shared representations) that generalise across languages, especially useful for languages with limited resources.</span></p>
</li>
<li>
<p>Combines a <strong>CNN</strong> for acoustic encoding with a <strong>transformer</strong> for contextual reasoning.</p>
</li>
<li>
<p>Used in <strong>modern ASR pipelines</strong>, and compatible with <strong>foundational audio models</strong> such as Whisper.</p>
</li>
</ol>
<h3 id="sample-representations-and-applications">Sample Representations and Applications</h3>
<p>Many modern speech representations are generated via <strong>self-supervised learning</strong> and used across a wide range of downstream applications. A unified interface allows these upstream models to be applied easily to different tasks.</p>
<p>Source: <a href="https://github.com/s3prl/s3prl">https://github.com/s3prl/s3prl</a></p>
<h3 id="chatgpt">ChatGPT</h3>
<p>OpenAI's ChatGPT has integrated <strong>voice and image capabilities</strong>, allowing natural voice conversations.</p>
<p>Key Technologies</p>
<ol>
<li><strong>Text-to-Speech (TTS)</strong>: New model generates human-like speech from just text and a few seconds of sample speech.</li>
<li><strong>Voice Acting</strong>: Professional voice actors were used to provide natural samples.</li>
<li><strong>Speech-to-Text (ASR)</strong>: Powered by <strong>Whisper</strong>, OpenAI's open-source ASR system.</li>
</ol>
<h3 id="llms-with-speech-recognition-capabilities">LLMs with Speech Recognition Capabilities</h3>
<p><strong>How it Works</strong></p>
<p>LLMs can be converted into ASR systems by prepending <strong>audio embeddings</strong> to the input text <strong>token embeddings</strong>, enabling them to process and generate output directly from auditory inputs.</p>
<p><strong>What is the Performance?</strong></p>
<p>Adding a <strong>conformer encoder</strong> to the LLaMA-7B allows it to outperform monolingual baselines by 18% and perform multilingual speech recognition despite LLaMA being only on English.</p>
<p><strong>Architecture Details</strong></p>
<p>Raw audio is first converted into filterbank features, which are then passed through a CNN followed by a conformer encoder. The encoder outputs are projected to match the LLM's embeddings dimensions, and embeddings are stacked (e.g., 3x stacking results in one embedding every 240 milliseconds).</p>
<blockquote>
<p><img src="file:////Users/naryaong/Code/git/ngpeijiun.github.io/ais/irs/_media/irs-5-conformer-encoder.png" alt="ASR with Conformer Encoder"></p>
</blockquote>
<p>The output from the audio encoder is prepended to the text token embeddings, and the entire sequence is fed into a frozen or LoRA-adapted decoder-only LLM. This setup enables joint audio-text processing for speech-to-text tasks.</p>
<blockquote>
<p><img src="file:////Users/naryaong/Code/git/ngpeijiun.github.io/ais/irs/_media/irs-6-joint-audio-text-embeddings.png" alt="Joint Audio-Text Embeddings"></p>
</blockquote>
<p>Source: Yassir Fathullah et al., <em>Prompting Large Language Models with Speech Recognition Abilities</em></p>
<h3 id="salmonn">SALMONN</h3>
<p>SALMONN (Speech Audio Language Music Open Neural Network) is a multimodal model designed for understanding and generating language grounded in speech, audio, and music inputs. It combines the Whisper speech encoder, the BEATs audio and music encoder, and a Q-Former module that fuses auditory features. The model then leverages a large language model (LLM) with LoRA adaptation to generate text responses based on prompts that may include audio, music, or language inputs.</p>
<blockquote>
<p><img src="file:////Users/naryaong/Code/git/ngpeijiun.github.io/ais/irs/_media/irs-7-salmonn.png" alt="SALMONN"></p>
</blockquote>
<p>Source: <a href="https://github.com/bytedance/SALMONN">https://github.com/bytedance/SALMONN</a></p>
<h3 id="audio-and-music-generative-models">Audio and Music Generative Models</h3>
<p><strong>AudioGen</strong></p>
<ol>
<li>Supports text-to-audio generation.</li>
<li>Trained on 4000 hours of 16 kHz audio data.</li>
<li>Source: <a href="https://felixkreuk.github.io/audiogen/">https://felixkreuk.github.io/audiogen/</a></li>
</ol>
<p><strong>AudioLDM2</strong></p>
<ol>
<li>Supports text-to-audio, text-to-speech, and image-to-audio.</li>
<li>Source: <a href="https://audioldm.github.io/audioldm2/">https://audioldm.github.io/audioldm2/</a></li>
</ol>
<p><strong>MusicGen</strong></p>
<ol>
<li>State-of-the-art controllable text-to-music model.</li>
<li>Source: <a href="https://ai.honu.io/papers/musicgen/">https://ai.honu.io/papers/musicgen/</a></li>
</ol>
<p><strong>MAGNeT</strong></p>
<ol>
<li>State-of-the-art non-autoregressive model for text-to-music and text-to-sound.</li>
<li>Source: <a href="https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT/">https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT/</a></li>
</ol>
<p><strong>AudioCraft</strong></p>
<ol>
<li>PyTorch library to use AudioGen, MusicGen, and MAGNeT.</li>
<li>Source: <a href="https://github.com/facebookresearch/audiocraft">https://github.com/facebookresearch/audiocraft</a></li>
</ol>

            <script async src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
            
        </body>
        </html>