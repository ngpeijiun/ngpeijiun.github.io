<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>Vision Cognition</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 16px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h1 style="color: #ccc">Intelligent Reasoning System</h1>
<h1 id="vision-cognition">Vision Cognition</h1>
<div class="badge">
    <span class="key">Type</span>
    <span class="value">Course</span>
</div>
<div class="badge">
    <span class="key">Instructor</span>
    <span class="value">Tian Jing</span>
</div>
<div class="badge">
    <span class="key">Institution</span>
    <span class="value">NUS-ISS</span>
</div>
<div class="badge">
    <span class="key">Note Updated</span>
    <span class="value">2025-06-27</span>
</div>
<h2 id="cognition">Cognition</h2>
<p><strong>Cognition</strong> is the mental action or process of acquiring knowledge and understanding through:</p>
<ol>
<li><strong>Senses</strong><br>
Processing input from sight, hearing, touch, smell, and taste.</li>
<li><strong>Experience</strong><br>
Learning from interactions and outcomes.</li>
<li><strong>Thought</strong><br>
Reasoning, problem-solving, decision-making.</li>
</ol>
<blockquote>
<p>&quot;Cognition is the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses&quot; — Quoted from Wikipedia</p>
</blockquote>
<h2 id="vision-cognition-1">Vision Cognition</h2>
<h3 id="marrs-model">Marr's Model</h3>
<p>Based on David Marr's Vision: <em>A Computational Investigation into Human Representation and Processing of Visual Information (1982)</em>, vision processing involves three sequential stages:</p>
<ol>
<li><strong>Early Vision</strong> → Raw feature detection<br>
Detects basic object <em>contours</em> and <em>edges</em>.</li>
<li><strong>Perception</strong> → Structured visual representation<br>
Constructs <em>2.5D</em> sketches: <em>surface orientations</em> from a specific viewpoint.</li>
<li><strong>Cognition</strong> → Semantic understanding of environment<br>
Interprets structured data using <em>background knowledge</em> to understand <em>3D</em> objects.</li>
</ol>
<h3 id="cognitive-vision-systems">Cognitive Vision Systems</h3>
<p>According to ECVision and modern cognitive vision literature, a cognitive vision system performs the following capabilities:</p>
<ol>
<li><strong>Detection</strong><br>
Identify objects or events.</li>
<li><strong>Localisation</strong><br>
Determine spatial positions.</li>
<li><strong>Recognition</strong><br>
Classify and match known entities.</li>
<li><strong>Understanding</strong><br>
Interpret meaning in context.</li>
</ol>
<p>Additional abilities:</p>
<ol>
<li><strong>Goal-directed behaviour</strong></li>
<li><strong>Adaptation</strong> to unforeseen visual changes</li>
<li><strong>Anticipation</strong> of object or event occurrence</li>
</ol>
<p>Achieved through:</p>
<ol>
<li><strong>Semantic knowledge learning</strong> (form, function, behaviour)</li>
<li><strong>Knowledge retention</strong> about environment and self</li>
<li><strong>Deliberation</strong> (reasoning) over objects, events, and context</li>
</ol>
<p>References</p>
<ol>
<li>Christensen &amp; Nagel (2006), <em>Cognitive Vision Systems</em>, Springer</li>
<li>Ghosh (2020), <em>Computational Models for Cognitive Vision</em>, Wiley</li>
</ol>
<h2 id="knowledge-from-vision-data">Knowledge from Vision Data</h2>
<h3 id="semantic-dimensions-of-knowledge">Semantic Dimensions of Knowledge</h3>
<p>Vision data encodes different types of knowledge across two dimensions:</p>
<ol>
<li>Semantic Depth
<ul>
<li><strong>Surface Semantic</strong><br>
Identifiable features</li>
<li><strong>Deep Semantic</strong><br>
Contextual, inferred meaning</li>
</ul>
</li>
<li>Temporal Nature
<ul>
<li><strong>Static</strong><br>
Unchanging facts</li>
<li><strong>Dynamic</strong><br>
Time-based context and actions</li>
</ul>
</li>
</ol>
<h3 id="types-of-knowledege">Types of Knowledege</h3>
<ol>
<li><strong>Entity Knowledge</strong> (Surface, Static)<br>
Information about discrete, identifiable objects or concepts, including attributes, properties, and classifications.<br>
Examples: Recognising a tree, car, or person along with their colour, shape, or size.</li>
<li><strong>Relational Knowledge</strong> (Deep, Static)<br>
Understanding connections, spatial arrangement, or contextual relationships between entities.<br>
Examples: A person sitting on chair; the relative position of buildings.</li>
<li><strong>Event Knowledge</strong> (Surface, Dynamic)<br>
Captures context and actions—what is happening, when, and where.<br>
Examples: Two people shaking hands; a car turning left at a junction.</li>
<li><strong>Common Sense Knowledge</strong> (Deep, Static/Dynamic)<br>
General world knowledge humans use to interpret scenes.<br>
Examples: Knowing that a cup is on a table and not floating in air.</li>
<li><strong>Procedural Knowledge</strong> (Deep, Dynamic)<br>
Sequences of actions to achieve a goal.<br>
Examples: Steps for handwashing; how to assemble furniture.</li>
</ol>
<h3 id="visual-summary">Visual Summary</h3>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">Static</th>
<th style="text-align:left">Dynamic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>Factual Knowledge</strong></td>
<td style="text-align:left">Entity Knowledge</td>
<td style="text-align:left">Event Knowledge</td>
</tr>
<tr>
<td style="text-align:left"><strong>Common Knowledge</strong></td>
<td style="text-align:left">Relational Knowledge, Common Sense Knowledge</td>
<td style="text-align:left">Procedural Knowledge</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Factual Knowledge</strong> (internal)<br>
Instance-level facts directly extracted from input vision data.</li>
<li><strong>Common Knowledge</strong> (external)<br>
Generalised patterns learnt from historical interaction with the world.</li>
</ul>
<h2 id="marrs-three-levels-of-vision">Marr's Three Levels of Vision</h2>
<ol>
<li><strong>Early Vision</strong><br>
<ul>
<li>Low-level image processing (e.g., edges, contours, colours, angles).</li>
<li>Pixel-based representation using colour or grayscale blocks, with each pixel encoded as numerical values.</li>
</ul>
</li>
<li><strong>Perception</strong> (Recognition)<br>
<ul>
<li>Intermediate-level image processing (object and shape recognition).</li>
<li>Image classification, object detection, and image segmentation.</li>
</ul>
</li>
<li><strong>Cognition</strong><br>
<ul>
<li>High-level understanding and reasoning from visual data.</li>
<li>Image captioning and optical character recognition (OCR)</li>
</ul>
</li>
</ol>
<h3 id="image-classification">Image Classification</h3>
<ol>
<li>Image-Level Task: Assign a label to an entire image.</li>
<li>Example: Label &quot;<em>Egyption cat</em>&quot; with confidence score.</li>
</ol>
<h3 id="object-detection">Object Detection</h3>
<ol>
<li>Block-Level Task: Identify and localise objects using bounding boxes and labels.</li>
<li>Example: Detect a &quot;<em>cat</em>&quot; with a bounding box.</li>
</ol>
<h3 id="image-segmentation">Image Segmentation</h3>
<ol>
<li>Pixel-Level Task: Map each pixel in the image to an object class, producing a detailed object-wise partition of the image.</li>
<li>Example: Foreground cat pixels segmented from background.</li>
</ol>
<h3 id="image-captioning--ocr">Image Captioning / OCR</h3>
<ol>
<li>Cognitive-Level Task: Generate a natural language description from an image.</li>
<li>Example: &quot;<em>A herd of giraffes and zebras grazing in a field.</em>&quot;</li>
</ol>
<h2 id="image-augmentation-techniques">Image Augmentation Techniques</h2>
<p>From the paper, Shorten &amp; Khoshgoftaar (2023), <em>Image Augmentation Techniques for Deep Learning</em>, common augmentations by model include:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Model</th>
<th style="text-align:left">Techniques</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">AlexNet</td>
<td style="text-align:left">Translate, Flip, Intensity Change</td>
</tr>
<tr>
<td style="text-align:left">ResNet</td>
<td style="text-align:left">Crop, Flip, AutoAugment, Mixup</td>
</tr>
<tr>
<td style="text-align:left">YOLOv4</td>
<td style="text-align:left">Mosaic, Distortion, Flip, GridMask, StyleGAN</td>
</tr>
<tr>
<td style="text-align:left">Swin Transformer</td>
<td style="text-align:left">RandAugment, Mixup, CutMix, Random Erasing</td>
</tr>
</tbody>
</table>
<p>Challenges addressed:</p>
<ol>
<li><strong>Image Variations</strong> (illumination, occlusion, etc.)</li>
<li><strong>Class Imbalance</strong> (few samples per class)</li>
<li><strong>Domain Shift</strong> (train/test style mismatch)</li>
<li><strong>Overfitting/Data Remembering</strong> (e.g., model memorises training examples)</li>
</ol>
<h2 id="vision-foundation-models">Vision Foundation Models</h2>
<p>In practice, we do not train computer vision models from scratch. Instead, we use <strong>pre-trained vision foundation models</strong> (e.g., CLIP, DINO, SAM) and <em>fine-tune</em> them to specific tasks. This saves time, reduces compute costs, and benefits from large-scale pretraining on diverse datasets.</p>
<h2 id="model-lifecycle">Model Lifecycle</h2>
<ol>
<li><strong>Data Phase</strong>
<ul>
<li><strong>Data Collection</strong><br>
Gather raw image data.</li>
<li><strong>Data Preparation</strong><br>
Clean, label, and augment images.</li>
</ul>
</li>
<li><strong>Training Phase</strong>
<ul>
<li><strong>Model Training</strong><br>
Train the model (or fine-tune a foundation model).</li>
<li><strong>Model Evaluation</strong><br>
Assess performance using metrics (e.g., accuracy, IoU).</li>
</ul>
</li>
<li><strong>Deployment Phase</strong>
<ul>
<li><strong>Model Deployment</strong><br>
Integrate into application or service.</li>
<li><strong>Model Validation</strong><br>
Test in real-world or production-like environments.</li>
</ul>
</li>
<li><strong>Maintenance Phase</strong>
<ul>
<li><strong>Model Monitoring</strong><br>
Track drift, errors, and user feedback.</li>
<li><strong>Model Update</strong><br>
Retrain or update with new data or better techniques.</li>
</ul>
</li>
</ol>

            
            
        </body>
        </html>