<h1 style="color: #ccc">Generative AI: Fundamentals to Advanced Concepts</h1>

# Programme Orientation

Mar 30, 2025

## Programme Outline

1.  Programme Details

    -   Overview of Python
        -   Understanding basics of Python syntax
        -   Create and use functions in Python
        -   Handle errors in Python
        -   Explore Object-Oriented Programming in Python
        -   Work with Python libraries such as Matplotlib and Pandas
    -   Machine Learning Fundamentals
        -   Understanding the fundamentals of Machine Learning
        -   Explore different Machine Learning approaches
        -   Differentiate between Supervised and Unsupervised Learning approaches
        -   Evaluate the advantages and disadvantages of Artificial Neural Networks
    -   Deep Learning Fundamentals and Introduction to Generative AI
        -   Understand the fundamentals of Deep Learning
        -   Describe the architecture of Deep Convolutional Neural Networks
        -   Apply DCNNs for Supervised Learning tasks
        -   Identify various Deep Learning architectures apart from DCNNs
        -   Describe how transformers are used in Generative AI
    -   Transformer Architecture and Attention Mechanisms
        -   Differentiate between transformers and traditional neural network architectures
        -   Identify the role of encoding and decoding in transformers
        -   Describe the mathematical foundations behind transformer models
        -   Analyse key components of transformer models, including embedding layers, self-attention mechanisms, multi-head attention and feedforward networks
        -   Prepare data for training transformer models
        -   Optimise transformer models for better performance
        -   Combine transformers with other Deep Learning architectures for specific outcomes
    -   Transformer-based Architecture (BERT and GPT)
        -   Compare the architectures of BERT and GPT models
        -   Explain the key components of BERT models, including token embedding, positional encoding and attention mechanisms
        -   Explain the role of Masked Language Model (MLM) and Next Sentence Prediction (NSP) in pre-training of BERT models
        -   Fine-tune GPT models for various Natural Language Processing tasks
        -   Utilise hybrid architectures to extend or combine BERT and GPT models for specific applications
    -   Reinforcement Learning for Generative AI
        -   Define value function, state-value function and policy in the context of Reinforcement Learning
        -   Explain the Exploration vs. Exploitation Dilemma and identify strategies to balance them effectively
        -   Use Bellman Equations to define optimal value functions
        -   Use Monte Carlo methods to estimate value functions based on episodic learning
        -   Use Q-Learning methods to find optimal action policies
        -   Describe how function approximation and policy gradient techniques are used in RL training
        -   Describe the RLHF pipeline used to train ChatGPT
        -   Use Direct Preference Optimisation to optimise human-aligned RL systems
    -   Early Image Generation Models (VAEs and GANs)
        -   Explain how Generative AI models differ from traditional AI models
        -   Describe the structure, functioning and applications of AutoEncoders
        -   Explain how Variational AutoEncoders use latent spaces to generate new data
        -   Describe the architecture of Gen AI networks and how they generate realistic data
        -   Train VAEs using KL Divergence and reparametrisation to improve performance
        -   Train GANs using techniques such as progressive growing, Wasserstein loss and spectral normalisation to prevent mode collapse and instability
        -   Use VAEs and GANs for image synthesis, super-resolution and style transfer and other real-world applications
    -   Image Generation using Diffusion Models
        -   Explain the fundamental principles behind diffusion processes and their role in generative AI
        -   Implement and optimise diffusion models for generating images using real-world datasets
        -   Apply appropriate sampling techniques to generate high-quality images from trained diffusion models
        -   Assess the performance of diffusion models using relevant metrics and improve model efficiency
        -   Explore practical applications of diffusion models in various domains, such as art, medical imaging, and data augmentation
    -   Transformer for Vision (ViT and CLIP)
        -   Differentiate between Transformer models and traditional CNNs with regards to Computer Vision applications
        -   Describe the structure, functioning and advantages of Vision Transformations (ViT) in image analysis
        -   Train and fine-tune ViT models for image classification and other vision tasks
        -   Describe how CLIP leverages constrastive learning to link images with text descriptions
        -   Use CLIP for tasks such as zero-shot classification, image retrieval and multi-modal understanding
        -   Assess the strengths and limitations of ViT and CLIP models
    -   Multi-modal Models: Text-to-Image
        -   Describe the fundamentals behind multi-modal AI
        -   Analyse the strengths, limitations and applications of leading text-to-image models
        -   Develop effective prompts to generate high-quality, contextually-relevant images
        -   Utilise APIs for seamless integration and adapt models for specific use cases
        -   Assess the ethical implications, limitations and practical applications of text-to-image AI
    -   Generative AI Applications: LLMs and RAG
        -   Explain the fundamental principles and architecture of LangChain
        -   Use LangChain to develop a simple conversational AI system
        -   Design and execute task-specific chains for efficient AI workflows
        -   Enhance AI responses by integrating external information retrieval systems
        -   Utilise embeddings for document search and retrieval to improve AI performance
    -   Generative AI Applications: Multi-agent Systems
        -   Explain the core concepts, types and communication protocols of multi-agent systems
        -   Apply techniques for effective communication and coordination among multiple agents
        -   Explain how reinforcement learning is applied to multi-agent environments
        -   Leverage LLMs for task planning, execution and autonomous problem-solving
        -   Assess the limitations, challenges, and emerging trends in multi-agent AI systems
