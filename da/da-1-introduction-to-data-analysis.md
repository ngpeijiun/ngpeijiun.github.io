<h1 style="color: #ccc">Data Analysis</h1>

# Introduction to Data Analysis

Jul 26, 2024

## What is Data Analysis?

### Introduction to Modern Data Ecosystem

1.  A modern data ecosystem includes a network of interconnected and continually evolving entities that include:

    -   Data integrated from disparate sources
    -   Various types of analysis and skills required to generate insights
    -   Active collaboration among stakeholders to act on generated insights
    -   Tools, applications, and infrastructure to store, process, and disseminate data as needed

2.  Data sources

    Data is available in a variety of structured and unstructured datasets from diverse and dynamic sources.

    -   **Step 1** <br> Pull a copy of the data from the original sources into a data repository. At this stage, you are only acquiring the data you need, working with data formats, sources, and interfaces through which this data can be pulled in. <br> **Challenges**: Reliability, security, and integrity of data.

    -   **Step 2** <br> Raw data needs to be organised, cleaned up, optimised for access, and conform to compliances and standards enforced in the organisation. For example, conforming to guidelines that regulate the storage and use of personal data, such as health, biometrics or household data in the case of IoT devices. Another example is adhering to master data tables within the organisation to ensure standardisation of master data across all applications and systems. <br> **Challenges**: Data management, repositories that provide high availability, flexibility, accessibility, and security.

    -   **Step 3** <br> Finally, business stakeholders, applications, programmers, analysts, and data science use cases pull data from the enterprise data repository. <br> **Challenges**: User interfaces, APIs, and applications.

3.  Emerging technologies shaping the modern data ecosystem

    -   **Cloud computing** <br> Every enterprise today has access to limitless storage, high-performance computing, open-source technologies, machine learning technologies, and the latest tools and libraries.

    -   **Machine learning** <br> Data scientists are creating predictive models by training machine learning algorithm on past data.

    -   **Big data** <br> Datasets today are so massive that traditional tools and analysis methods are no longer adequate, paving way for new tools, techniques, and insights.

4.  Organisations are using data to uncover opportunities and applying that knowledge to differentiate themselves from their competition.

    -   Identifying patterns in financial data to detect fraud
    -   Using recommendation engines to drive conversions
    -   Mining social media posts for customer insights
    -   Analysing customer behaviour to personalise offers

### Data Professionals

5.  Data professionals

    -   Data engineers
    -   Data analysts
    -   Data scientists
    -   Business analysts
    -   Business intelligence analysts

6.  **Data engineers** develop and maintain data architectures, ensuring that the data is available and accessible in formats and systems that various business applications and stakeholders, such as data analysts and data scientists, can utilise for operations and analysis. They convert raw data into usable data, enabling its effective use across different platforms and by various users.

    Responsibilities of a **data engineer**:

    -   Extract, integrate, and organise data from disparate sources.
    -   Clean, transform, and prepare data.
    -   Design, store, and manage data in data repositories.

    Skills:

    -   Good knowledge of programming
    -   Sound knowledge of systems and technology architectures
    -   In-depth understanding of relational databases and non-relational data stores

7.  **Data analysts** and **data scientists** translate data and numbers into plain language, enabling organisations to make informed decisions. Data analysts use this data to generate insights, while data scientists build models to predict the future using data from the past.

    Responsibilities of a **data analyst**:

    -   Inspect and clean data to derive insights.
    -   Identify correlations, find patterns, and apply statistical methods to analyse and mine data.
    -   Visualise data to interpret and present the findings of data analysis.

    Skills:

    -   Proficiency in spreadsheets, writing queries, and using statistical tools to create charts and dashboards
    -   Programming skills
    -   Strong analytical and storytelling skills

    Responsibilities of a **data scientist**:

    -   Analyse data for actionable insights.
    -   Create predictive models using machine learning and deep learning techniques.

    Skills:

    -   Knowledge of mathematics and statistics
    -   Understanding of programming languages, databases, and building data models

8.  **Business analysts** leverage the work of data analysts and data scientists to evaluate possible implications for their business and determine or recommend necessary actions. They use these insights and predictions to drive decisions that benefit and grow their business.

9.  **Business intelligence analysts** focus on market forces and external influences that shape their business. They organise and monitor data across different business functions, and explore data to extract insights and actionable strategies to improve business performance.

### Data Analysis

1.  Data analysis is the process of gathering, cleaning, analysing, and mining data, interpreting results, and reporting the findings. Through data analysis, we find patterns within data and correlations between different data points. It is through these patterns and correlations that insights are generated, and conclustions are drawn. Data analysis helps businesses understand their past performance and informs their decision-making for future actions. By using data analysis, businesses can validate a course of action before committing to it, saving valuable time and resources and ensuring greater success.

2.  Different types of data analysis

    **Descriptive analytics**
    -   What happened
    -   Provides insights into past events

    **Diagnostic analytics**
    -   Why did it happen
    -   Takes the insights from descriptive analytics to dig deeper to find the cause of the outcome

    **Predictive analystics**
    -   What might happen next
    -   Leverages historical data and trends to predict future

    **Prescriptive analytics**
    -   What should be done about it
    -   Analyses past decisions and events to estimate the likelihood of different outcomes

3.  Data analysis process

    -   **Understanding the problem and desired result**
    -   **Setting a clear metric**
    -   **Gathering data**
    -   **Cleaning data**
    -   **Analysing and mining data**
    -   **Interpreting results** <br> As you interpret your results and confirm your hypothesis, you need to evaluate if your analysis is defensible against objections, and if there are any limitations or circumstances under which your analysis may not hold true.
    -   **Presenting your findings** <br> The art of storytelling with data is as crucial as the analysis itself. Being able to communicate and present your findings in clear and impactful ways ensure that your insights are understood and can drive meaningful action, enabling informed decision-making based on the data analysis.

4.  Data analysis and data analytics are often used interchangably.

### Data Analyst

1.  Responsibilities of a Data Analyst

    -   **Acquiring data** from varied primary and secondary data sources that best serve the use case
    -   **Creating queries** to extract required data from databases, data repositories, and other data collection systems
    -   **Filtering, cleaning, standardising, and reorganising data** in preparation for data analysis
    -   **Using statistical tools** to analyse data and understand what it represents
    -   **Using statistical techniques** to identify patterns, correlations, and insights in data
    -   **Analysing patterns** in complex datasets and interpreting trends
    -   **Preparing reports and charts** that effectively communicate trends, patterns and insights to stakeholders who need to act on the findings
    -   **Creating appropriate documentation** to define and demonstrate the steps of the data analysis process for future reference and repeatability
    -   **Interacting with stakeholders** to gather information and present findings

2.  Technical skills

    -   **Expertise in using spreadsheets**: Microsoft Excel or Google Sheets
    -   **Proficiency in statistical analysis and visualisation tools and software**: IBM Cognos, IBM SPSS, Oracle Visual Analyser, Microsoft Power BI, SAS, and Tableau
    -   **Proficiency in programming languages**: Python, Java
    -   **Good knowledge in SQL and ability to work with data in relational and NoSQL databases**
    -   **The ability to access and extract data from data repositories**: Data Marts, Data Warehouses, Data Lakes, and Data Pipelines
    -   **Familiarity with Big Data processing tools**: Hadoop, Hive, and Spark

3.  Functional skills

    -   **Proficiency in statistics** <br> Analyse data, validate the analysis, identify fallacies and logical errors.
    -   **Analytical skills** <br> Research and interpret data, theorise, make forecasts.
    -   **Problem-solving skills** <br> Come up with possible solutions for a given problem.
    -   **Probing skills** <br> Identify and define the problem statement and desired outcome.
    -   **Data visualisation skills** <br> Create clear and compelling visualisations to present the analysis.
    -   **Project management skills** <br> Manage the process, people, dependencies, and timelines.

4.  Soft skills

    -   Work collaboratively with business and cross-functional teams.
    -   Communicate effectively to report and present your findings.
    -   Tell a compelling and convincing story.
    -   Gather support and buy-in for your work.

5.  Curiosity

    Allowing new questions to surface and challenging your own assumptions and hypotheses.

6.  Intuition

    Having a sense of the future based on pattern recognition and past experiences.

## Data Ecosystem

### Data Types, Formats, and Sources

1.  A data ecosystem includes the infrastructure, software, tools, frameworks, and processes used to collect, store, clean, visualise, and analyse data.

2.  Data can be classified into three categories: structured, semi-structured, and unstructured.

    -   **Structured data** is organised in a rigid format, easily fitting into rows and columns, like in databases and spreadsheets, and is suitable for standard analysis methods.
    -   **Semi-structured data** combines consistent characteristics with non-rigid structure, using meta tags and attributes for organisation, commonly seen in formats like XML and JSON. An example is an email, which has both structured (sender and recipient names) and unstructured (email content) elements.
    -   **Unstructured data** consists of complex, qualitative information that does not fit neatly into rows and columns, such as photoes, videos, text files, PDFs and social media content.

3.  Understanding different file formats

    -   **Delimited text file format (.csv)** <br> Used for data stored in plain text, separated by delimiters like commas.
    -   **Microsoft Excel Open XML Spreadsheet (.xlsx)** <br> Format for Excel spreadsheets.
    -   **Extensible Markup Language (.xml)** <br> Used for encoding documents in a format readable by both humans and machines.
    -   **JavaScript Object Notation (.json)** <br> Lightweight format for data interchange, easy for human to read and write, and easy for machine to parse and generate; commonly used by APIs and Web Services.
    -   **Portable Document Format (.pdf)** <br> Ensures consistent viewing across all devices.

4.  Common sources of data

    -   **Relational databases** <br> Examples include Oracle, MySQL, MariaDB, Microsoft SQL Server, PostgreSQL and IBM Db2.
    -   **Flat files and XML datasets** <br> Examples include CSV, spreadsheet, and XML files.
    -   **APIs and web services** <br> Provides data in formats like plain text, XML, HTML, JSON, or media files.
    -   **Web scraping** <br> Extracts specific information from web pages using tools like BeautifulSoup, Scrapy, Pandas, and Selenium.
    -   **Data stream and feeds** <br> Aggregate data from instruments, IoT devices, GPS data, social media posts, often timestamped and geo-tagged. Tools for processing include Apache Kafka, Apache Spark Streaming, and Apache Storm. RSS feeds capture updated data from online forums and news sites.

5.  Examples of data stream use cases

    -   Stock and market tickers for financial trading
    -   Retail transaction streams for predicting demand and managing supply chain
    -   Surveillance and video feeds for threat detection
    -   Social media feeds for sentiment analysis
    -   Sensor data feeds for monitoring industrial or farming machinery
    -   Web click feeds for monitoring web performance and improving design
    -   Real-time flight events for rebooking and rescheduling

6.  The types, formats, and sources of data determine the appropriate **data repositories** for collection and storage, as well as the **tools** for processing and querying the data.

    If you are working with big data, for example, you will need big data warehouses to store and process high-volume, high-speed data, as well as frameworks to perform complex real-time analytics.

7.  Languages available in data ecosystem

    -   **Query languages** <br> SQL for querying and manipulating data
    -   **Programming languages** <br> Python for developing data applications, with libraries such as Pandas (data cleaning and analysis), Numpy and Scipy (statistical analysis), Beautiful Soup and Scrapy (web scraping), Matplotlib and Seaborn (data visualisation), and Open CV (image processing).
    -   **Shell and scripting languages** <br> Shell for automating repetitive operational tasks.

### Data Repositories

1.  A **data repository** is a collection of organised and isolated data designed to improve the efficiency and credibility of reporting and analysis, while also serving as a data archive.

2.  A **database management system** (DBMS) is a software that creates and maintains databases. It allows you to store, modify, and retrieve information from the databases using query functions.

3.  **Relational databases** (RDBMS) organise data into tables with rows and columns following a defined structure and schema. SQL (Structured Query Language) is the standard language for querying these databases, enabling fast processing of millions of records. Popular RDBMS include Oracle, MySQL, MariaDB, Microsoft SQL Server, PostgreSQL, and IBM Db2.

    **Cloud relational databases** (DBaaS) are widely used for their access to limitless cloud compute and storage capabilities. Popular DBaaS offerings include Amazon Aurora, Amazon RDS, Google Cloud SQL, Azure SQL Database, Oracle Cloud, and IBM Db2 on Cloud.

4.  **Non-relational databases** (NoSQL databases) offer schema-less data storage and retrieval, gaining popularity with cloud computing, big data, and high-speed web and mobile applications due to their scalability, performance, and ease of use.

    Four common types of NoSQL databases:

    -   **Key-value stores** <br> Data is stored as key-value pairs. Popular key-value stores include Redis, Amazon DynamoDB, Microsoft Azure Cosmos DB, and Memcached.
    -   **Document stores** <br> Data and associated information are stored within a single document. Popular document stores include MongoDB, CouchDB, IBM Cloudant, and Amazon DocumentDB.
    -   **Wide column stores** <br> Data is stored in columns instead of rows. Columns accessed together are grouped into a column family. This method allows for fast data access and search since all cells of a column are stored continuously on disk. Popular wide column stores are Cassandra and HBase.
    -   **Graph databases** <br> Data is stored in a graphical model, ideal for handling interconnected data. They excel in visualising, analysing, identifying relationships in data, making them suitable for social networks, real-time recommendations, network diagrams, fraud detection, and access management. However, they are not optimised for high-volume transaction processing. Popular graph databases include Neo4j and Microsoft Azure Cosmos DB.

5.  Relational databases are ideal for handling large volumes of structured data with complex relationships, where data consistency is crucial. Key benefits include:

    -   **Consistency** <br> Ensure data reliability and integrity through ACID properties, maintaining a consistent database state.
    -   **Availability** <br> Provide reliable query responses, keeping the system operational and accessible within the same server cluster or geographical proximity.

    These features make relational databases ideal for Online Transaction Processing (OLTP) systems, supporting high-speed transactions. To avoid disrupting OLTP transactions, data is transferred to a data warehouse or Online Analytical Processing (OLAP) database for analysis.

6.  NoSQL databases are ideal for handling large volumes of structured, semi-structured, and unstructured data. Key benefits include:

    -   **Availability** <br> Ensure continuous operation and data access even during node failures or network issues.
    -   **Partition tolerance** <br> Maintain functionality and data accessibility across distributed systems during network partitions.

    These features make NoSQL databases perfect for high-volume, high-speed data inflows across wide geographical regions, where network partitions are common. Direct data analysis on NoSQL databases is more practical than transferring data to another data warehouse for analysis.

7.  A **data warehouse** is a central data repository that consolidates data from various sources through the ETL process. It stores current and historical data for reporting and analysis, serving as a single source of truth.

8.  A **data mart** is a subset of data warehouse designed for specific business functions or user communities. It provides targeted data and analytical capabilities, ensuring isolated security and performance, mainly used for business-specific reporting and analytics.

9.  A **data lake** is a storage repository that holds large amounts of structured, semi-structured, and unstructured data in their native formats, tagged with metadata. Unlike a data warehouse, which stores processed data for specific uses, a data lake retains all raw data without exclusions. Data lakes area ideal for handling large volumes of continuously generated data without predefined use cases and also serve as staging areas for data warehouses.

10. **ETL** (Extract, Transform, Load) is an automated process for converting raw data into analysis-ready data. It involves:

    -   **Extract** <br> Collecting data from various sources for transformation. This can be done via:
        -   **Batch processing** <br> Moving large chunks of data at scheduled intervals using tools like Stitch and Blendo.
        -   **Stream processing** <br> Pulling data in real-time and transforming it during transit with tools like Apache Samza, Apache Storm, and Apache Kafka.

    -   **Transform** <br> Converting raw data into a usable format by standardising formats, removing duplicates, filtering unnecessary data, enriching data, and applying business rules.

    -   **Load** <br> Transporting processed data to a data repository. This step includes initial loading, incremental loading, or full refresh, along with load verification to check for missing values, server performance, and monitoring failures.

11. **Data pipeline** and ETL are often used interchangeably, but they have some notable differences:

    -   ETL is typically used for structured data, whereas a data pipeline can handle structured, semi-structured, and unstructured data.
    -   ETL usually processes data in batches, while a data pipeline can move data in real-time streams.
    -   ETL ends after loading the data, but in a data pipeline, the loading process can trigger additional processes or workflows.

### Big Data

1.  **Big Data** refers to the vast and varied amounts of data generated by people, tools, and machines. This data requires innovative and scalable technology to collect, store, and analyse it. The goal is to derive real-time business insights on consumer, risk, profit, performance, productivity, and shareholder value.

2.  There is no single definition of big data, but it generally includes elements like velocity, volume, variety, veracity, and value.

    -   **Velocity** <br> Velocity refers to the speed at which data is generated and accumulates. Data is produced at an extremely rapid and continuous rate. Technologies for near or real-time streaming, whether local or cloud-based, can process this information quickly.
    -   **Volume** <br> Volume refers to the scale or the amount of data being stored. The increase in data volume is driven by more data sources, higher resolution sensors, and scalable infrastructure.
    -   **Variety** <br> Variety refers to the diversity of data, which includes structured, semi-structured and unstructured data. It also indicates that data comes from various sources, such as machines and people, both internal and external to organisations. The increase in data variety is driven by mobile technologies, social media, wearable technologies, geo technologies, video, and many other sources.
    -   **Veracity** <br> Veracity is the quality and origin of data and its conformity to facts and accuracy. Attributes include consistency, completeness, integrity, and ambiguity. Drivers: cost and the need for traeability. With the large amount of data available, the debate rages on about the accuracy of data in the digital age. Is this information real or is it false?
    -   **Veracity** <br> Veracity refers to the quality and origin of data, ensuring it is factual and accurate. Essential attributes include consistency, completeness, integrity, and clarity. Veracity is influenced by cost and the need for traceability. In the digital age, the vast amount of data has led to ongoing debates about its accuracy and authenticity.
    -   **Value** <br> Value is our ability and need to transform data into meaningful benefits. It encompasses more than just profit, including medical and social advantages, as well as customer, employee, and personal satisfaction. The primary reason people invest time in understanding big data is to derive value from it.

3.  Three open source technologies widely used in big data analytics are Apache Hadoop, Apache Hive, and Apache Spark.

4.  **Apache Hadoop**, a java-based open-source framework, allows distributed storage and processing of large datasets across clusters of computers.

    In a Hadoop distributed system, a node is a single computer, and a collection of nodes forms a cluster. Hadoop can scale up from a single node to any number of nodes, each offering local storage and computation.

    HDFS (Hadoop distributed File System) is a key Hadoop component that provides scalable and reliable storage for big data across multiple nodes. It partitions large files over several computers, enabling parallel access and computations. HDFS also replicates file blocks on different nodes, ensuring fault tolerance and preventing data loss.

    Hadoop provides a reliable, scalable, and cost-effective solution for storing data with no format requirements. Benefits include:

    -   **Better real-time data-driven decisions** <br> Incorporates emerging data formats not traditionally used in data warehouses.
    -   **Improved data access and analysis** <br> Provides real-time, self-service access to stakeholders.
    -   **Data offload and consolidation** <br> Optimises and streamlines costs by consolidating data across the organisation and moving cold data, that is, data not in frequent use, to a Hadoop-based system.

5.  **Apache Hive** is an open-source data warehouse software for managing large datasets stored in HDFS or Apache HBase, providing SQL-based tools for easy data access.

    However, since Hadoop is based on long sequential scans, Hive quries tend to have high latency. This makes Hive less suitable for applications requiring very fast response times and not ideal for transaction processing that involves a high percentage of write operations.

    Hive is better suited for data warehousing tasks like ETL, reporting, and data analysis.

6.  **Apache Spark** is a distributed data analytics framework designed for real-time, complex data analytics.

    By using in-memory processing, Spark achieves high computation speeds, spilling to disk only when memory is constrained. It supports Java, Scala, Python, R, and SQL, and can run on its standalone clustering technology or on Hadoop, accessing data from sources like HDFS and Hive.

    Spark's key strength is its ability to handle streaming data and perform real-time analytics efficiently.

## Data Collection

1.  Defining objectives

    The data analysis process begins with understanding the problem and desired outcome.

    Clearly outline your goal by understanding the problems you are addressing and identifying your desired outcomes. By pinpointing the precise information required, you ensure that the data collection and analysis process is structured effectively, helping you focus on relevant data.

    Establish a detailed plan for data collection. This involves setting a specific timeframe, which could be ongoing or limited to a defined period, such as tracking a particular event with a start and end date. Additionally, determine the scope of the data needed to ensure a robust and credible analysis. Consider the amount and variety of data required to reach a comprehensive and reliable conclusion.

2.  Selecting data sources

    Identify where the data will come from, which can be internal or external to the organisation. Data sources may include primary, secondary, or third-party sources.

    -   **Primary data sources** <br> Primary data sources are original, first-hand accounts or direct evidence concerning a topic. They are collected directly from the source and remain unfiltered by any other interpretation.
    -   **Secondary data sources** <br> Secondary data sources are derived from primary data sources and include analyses, interpretations, or summaries. They provide context, commentary, or evaluation rather than direct accounts of the original data. This type of data has already been collected by someone else and is available for use by others as a reinterpretation or summary of existing data.
    -   **Third-party data sources** <br> Third-party data sources are collected and aggregated by an entity that did not originally collect the data and is not the direct user of it. This data is usually purchased or licensed from an external organisation that specialises in the data collection and analysis.

    Examples of primary data sources include:

    -   Census
    -   Surveys
    -   Interviews
    -   Experiments
    -   Observations
    -   Official records or documents
    -   Web
    -   Sensors
    -   Social media

    Examples of secondary data sources include:

    -   Databases
    -   Research articles
    -   Industrial analyses
    -   Web

    Examples of third-party data sources include:

    -   Data exchanges
    -   Data brokers
    -   Syndicated data services

3.  Choosing data collection methods

    Selecting the right data collection method depends on the objectives and data sources. Key methods include traditional approaches like surveys, interviews, and observations, as well as modern techniques such as using query languages, APIs, web scraping, and data streams.

    -   **Query languages** <br> Query languages are tools used to retrieve and manipulate data from databases. SQL is specifically used for querying relational databases. Non-relational databases can also be queried using SQL or similar languages. Some non-relational databases have their own unique querying languages, like CQL for Cassandra and GraphQL for Neo4J.
    -   **APIs** <br> APIs enable data collection from different software systems, allowing seamless integration and automated retrieval. They facilitate communication between software applications using protocols like REST, SOAP, and GraphQL, making them popular for extracting data from various sources.
    -   **Web scraping** <br> Web scraping is an automated method for collecting large amounts of data from websites, especially when the data is not easily accessible. It uses tools like BeautifulSoup, Scrapy, and Selenium to extract this information.
    -   **Data streams** <br> Data streams involve continuously collecting real-time data from sources like social media, IoT devices, and financial markets, enabling timely analysis and decision-making.

4.  Desiging instruments

    To collect data effectively, you can create or select various tools like questionnaires, interview guides, and observation checklists. Additionally, you can develop software for automated data collection, including writing codes to extract data from databases, APIs, web scraping, and data streams.

5.  Importing data

    The importing process involves combining data from various sources to create a unified view and interface for querying and manipulation. Structured data is imported into relational databases optimised for OLAP analytics. Semi-structured and unstructured data are stored in NoSQL databases within operational systems. These databases can serve as data marts in the data warehouse for real-time analytics, where it is impractical to import vast amounts of real-time data into another database. Data without a specific use case is stored in a data lake, typically a NoSQL database or distributed file system, to handle all data types and schemas.

6.  The data you identify, the sources from which you collect it, and the methods you use for gathering have significant implications for various aspects such as:

    -   Data quality (including data usability, data integrity, and data accessibility)
    -   Data governance
    -   Data privacy

7.  Data quality metrics

    -   **Free of errors** <br> The data should be checked and verified to be free from mistakes.
    -   **Accurate** <br> Data should correctly represent the information it is supposed to.
    -   **Complete** <br> Ensure all necessary data is collected without missing crucial parts.
    -   **Relevant** <br> The data must be pertinent to the goal and analysis.
    -   **Accessible** <br> Data should be easily retrievable and usable by authorised personnel.

8.  Steps crucial for ensuring data quality:

    -   **Checks** <br> Regularly verify the data to ensure it meets quality standards.
    -   **Validations** <br> Validate data accuracy through cross-referencing and consistency checks.
    -   **Auditable trails** <br> Maintain detailed records of data collection and processing activities to ensure transparency and accountability.

9.  Data governance

    -   **Security** <br> Ensure the data is protected from unauthorised access and breaches.
    -   **Regulation** <br> Comply with relevant laws and regulations governing data use.
    -   **Compliance** <br> Adhere to internal policies and external mandates regarding data management.

10.  Data privacy

    -   **Confidentiality** <br> Ensure that sensitive information is only accessible to authorised individuals.
    -   **License for Use** <br> Obtain proper permissions and licenses for data usage.
    -   **Compliance with Mandated Regulation** <br> Adhere to legal requirements related to data privacy.

## Data Wrangling

1.  **Data wrangling** is an iterative process that converts raw data into analysis-ready data for credible and meaningful analysis. It typically involves a 4-step process:

    -   Discovery phase <br> Understanding the data you have and its context.
    -   Transformation phase <br> Converting data into a format suitable for analysis.
    -   Validation phase <br> Ensuring the transformed data is accurate and reliable.
    -   Publishing phase <br> Making the cleaned and validated data available for analysis and use.

2.  The **discovery phase**, or exploration phase, focuses on understanding your data in relation to your use case. The goal is to determine the best way to clean, structure, organise, and map the data for your specific needs.

3.  The **transformation phase** is the core of the data wrangling process, involving tasks like structuring, normalising, denormalising, cleaning, and enriching the data.

    -   **Structuring** <br> This involves changing the form and schema of data to ensure consistency and compatibility, especially when dealing with various sources like databases and web APIs. Actions include reordering fields, combining fields into complex structures, and performing joins and unions to merge data from multiple tables.
    -   **Normalising** <br> This involves cleaning databases by removing unused data and reducing redundancy and inconsistency, common in transactional systems with frequent insert, update, and delete operations.
    -   **Denormalising** <br> This combines data from multiple tables into a single table to improve query performance, often done with normalised data from transactional systems before reporting and analysis.
    -   **Cleaning** <br> This fixes data irregularities such as inaccuracies, missing or incomplete data, biases, null values, and outliers to ensure credible and accurate analysis.
    -   **Enriching** <br> This adds additional data points to make the analysis more meaningful, like tagging a public dataset with customer identifier from your organisation's customer master table.

4.  Data cleaning is a crucial part of the transformation phase. According to a Gartner report on data quality, poor-quality data can lead to false conclusions and, therefore, ineffective decisions, which can be costly in the business world.

    Data can have various issues, such as formatting issues, duplicates, missing values, inaccuracies, and outliers. If the data with issues cannot be repaired, it must be removed from the dataset.

    Types of data cleaning include:

    -   Formatting issues, such as extra white spaces, typos, and format inconsistencies, need to be fixed.
    -   Duplicate data points should be identified and removed from your dataset.
    -   Missing values can cause unexpected or biased results. To address this issue, you can filter out records with missing data, source missing information from external data sources, or impute the missing value based on interpolation or statistical values.
    -   Check if the values of the fields fall within the expected range, considering aspects such as zero or negative values, tolerance levels, and static checks. For example, outliers, which may or may not be correct, should be reviewed to determine their correctness and whether they should be included in the dataset.

    To ensure the reliability of your data, calculate and verify summary statistics for each column. These include the mean, median, mode, minimum, and maximum. Check these statistics for consistency with real-world expectations.

5.  The **validation phase** involves checking the quality of data after structuring, normalising, denormalising, and enriching. This phase uses validation rules, which are repetitive programming steps, to verify the consistency, quality, and security of the data.

6.  The **publishing phase** involves delivering the transformed, validated, and analysis-ready data, for downstream projects, including metadata about the dataset.

7.  Python provides powerful data manipulation capabilities through its extensive libraries and packages. Jupyter Notebook is a popular open-source web application for creating and sharing documents with live code, equations, visualisations, and narrative text, widely used in data analysis, scientific research, machine learning, and education. NumPy supports arrays, matrices, and a wide range of mathematical functions for efficient scientific computing, while Pandas offers robust data manipulation and analysis with structures like DataFrames and Series, aiding in data cleaning, transformation, and analysis.

## Analysis and Mining

### Statistical Analysis

1.  **Statistics** is a branch of mathematics that deals with the collection, analysis, interpretation, and presentation of quantitative data. It plays a crucial role in various industries today, enabling data-driven decision-making.

2.  **Statistical analysis** involves applying statistical methods to a sample of data to discover patterns and trends, develop an understanding of what the data represents, and draw conclusions from it.

    There are two main types of statistical analysis:

    -   **Descriptive statistics** <br> This involves summarising and describing information about a sample, without attempting to draw conclusions about the population from which the sample is taken. Descriptive statistics include measures of central tendency (mean, median, mode), measures of variability or dispersion (range, variance, standard deviation), measures of skewness (right skew, left skew, zero skew).
    -   **Inferential statistics** <br> This involves making inferences or generalisations about a population based on a sample of data. Techniques include hypothesis testing, confidence intervals, and regression analysis.

    The common order of steps in statistical analysis:

    -   **Collecting** <br> Gathering data from various sources, such as surveys, experiments, or databases.
    -   **Organising** <br> Structuring the collected data in a systematic manner, such as sorting, classifying, tabulating, and cleaning.
    -   **Analysing** <br> Applying statistical methods and techniques to the organised data to identify patterns, trends, and relationships.
    -   **Interpreting** <br> Making sense of the analysis results, drawing conclusions, and understanding the implications of the findings.
    -   **Presenting** <br> Communicating the interpreted results through reports, charts, graphs, and presentations to inform decision-making or further action.

### Data Mining

1.  Data mining is the process of extracting knowledge from data to identify patterns, trends, and correlations. Pattern recognition is the discovery of regularities or commonalities in data. Trend is the general tendency of a set of data to change overtime. Data mining has applications across industries and disciplines, such as customer profiling, fraud detection, healthcare prediction, and supply chain optimisation.

2.  Different types of data analytics

    **Descriptive analytics**
    -   What happened
    -   Provides insights into past events

    **Diagnostic analytics**
    -   Why did it happen
    -   Takes the insights from descriptive analytics to dig deeper to find the cause of the outcome

    **Predictive analystics**
    -   What might happen next
    -   Leverages historical data and trends to predict future

    **Prescriptive analytics**
    -   What should be done about it
    -   Analyses past decisions and events to estimate the likelihood of different outcomes

3.  Data mining techniques

    -   **Classification** <br> The process of assigning labels to data points based on their features, classifying data points into predefined target categories.
    -   **Clustering** <br> Clustering is the process of grouping similar data points together without using predefined labels. It organises data into clusters, enabling these groups to be treated collectively for analysis or further processing.
    -   **Anomaly or outlier detection** <br> This involves identifying data points that significantly deviate from the norm. It focuses on finding patterns in data that are unexpected or not typical.
    -   **Association rule mining** <br> This technique is used to discover relationships and patterns between variables in large datasets. It identifies associations between data events, demonstrating how the presence of one item can influence another. Commonly used in market basket analysis and other fields, it aids in data-driven decision-making.
    -   **Sequential patterns** <br> This involves identifying and tracing regular sequences or patterns in data over time. This process examines a series of events that occur in a specific order, helping to uncover trends, behaviours, or relationships within the data.
    -   **Affinity grouping** <br> Identifying sets of items that frequently occur together, often used in discovering co-occurence relationships, such as recommending products that are commonly bought together.
    -   **Decisoin trees** <br> Decision trees are models used for decision-making based on data features. They have a tree-like structure with branches representing possible outcomes. Each branch corresponds to a decision point or feature in the data, leading to a classification or prediction at the end. This method is used for building classification models to determine the likely occurence of different outcomes.
    -   **Regression** <br> This involves predicting a continuous outcome variable based on input features. It helps identify the nature of the relationship between two variables, which can be either causal or correlational.

## Communications

1.  As humans naturally understand the world through narratives, data gains value through the stories it tells.

2.  The data analysis process begins with understanding the problem that needs to be solved and the desired outcome. It ends with communicating the findings in ways that impact decision-making. The success of your communication depends on how well others can understand, relate to, and trust your insights to take further action. Data analysts must tell a compelling story with their data, using clear visualisations and a structured narrative tailored to the audience.

3.  What is important to your audience?

    You have to tell a compelling story based on your understanding of your audience. Frame your presentation to match their level of knowledge to better convey your findings.

    A presentation is not a data dump. Facts and figures alone do not drive decisions or motivate actions. While it is tempting to share all the data, prioritise what is most important to your audience. Overloading with information can confuse them, so only include what is necessary to address the business problem.

4.  What will help them trust me?

    Begin your presentation by demonstrating your understanding of the business problem to your audience. It is easy to fall back on the assumption that everyone knows why they are here for, but reflecting your understanding of the problem that needs to be solved and the outcome that needs to be achieved is a great first step in winning their attention and starting with trust. Speaking in the language of the organisation's business domain is another important factor in building a connection between you and your audience.

5.  Structure your presentation

    To structure your presentation for maximum impact, start by referencing and establishing the credibility of your data. Share your data sources, hypotheses, assumptions, and validations to build trust with your audience. Organise the information into a logical categories, using either a top-down or bottom-up approach as appropriate. Choose communication formats that will be most useful and comprehensible for your audience. Finally, ensure your insights are explained in a way that inspires action; if the audience does not grasp or believe in the significance and utility of your insights, they will not drive any value.

6.  The role of visuals

    A powerful visual can create a clearer mental image in the mind of your audience than a thousand-word essay,data visualisation tells stories through the graphical depiction of facts and figures, bringing data to life.

7.  Data visualisation is the discipline of communicating information through the use of visual elements to make it easy to comprehend, interpret, and retain.

    To create valuable data visualisations, you must choose the type that effectively communicates your findings to your audience based on the relationship you are trying to establish&mdash;whether comparing values, analysing single value over time, or detecting anomalies. Always ask yourself, "What is the question I am trying to answer?" and ensure your visualisations address this question for your audience. Additionally, consider whether a static or interactive visualisation is more appropriate. Interactive visualisations can allow users to change values and observe real-time effects on related variables.

    Consider the key takeaway for your audience, anticipate their information needs and the questions they may have, and then plan the visualisation to deliver your message clearly and impactfully.

8.  Common types of data visualisations

    -   **Bar chart** <br> `<label, value>` Used to compare different categories of data.
    -   **Pie chart** <br> `<label, value>` Shows proportions of a whole.
    -   **Tree map** <br> `<label, value>` Display hierarchical data with nested rectangles.
    -   **Radar chart** <br> `<label, value>` Comparing multiple variables.
    -   **Sunburst chart** <br> `<label, value>` Visualising hierarchical data with concentric circles.
    -   **Waterfall chart** <br> `<label, value>` Visualising the cumulative effect of sequential data.
    -   **Funnel chart** <br> `<stage, value>` Representing stages in a process.
    -   **Line chart** <br> `<label, datetime, value>` Useful for showing trends over time.
    -   **Area chart** <br> `<label, datetime, cumulative_value>` Showing cumulative data over time.
    -   **Scatter plot** <br> `<value, value>` Revealing relationships between two variables.
    -   **Bubble chart** <br> `<value, value, value>` Adding a third dimension to scatter plots with bubble size.
    -   **Box plot** <br> `<label, summaries>` Summarising the distribution of data, including median, quartiles, and outliers.
    -   **Histogram** <br> `<value, count>` Displays the distribution of a dataset.
    -   **Density chart** <br> `<value, KDE>` Showing the distribution of data using kernel density estimate.
    -   **Violin chart** <br> `<value, KDE>` Combining density plot with box plot, quartile lines, rug plot, jittered strip plot, or swarm plot.
    -   **Heatmap** <br> `<label, label, value>` Representing data values in a matrix with colours.
    -   **Correlagram** <br> `<label, label, correlation>` Displaying correlation matrix for variables.
    -   **Gantt chart** <br> `<label, start_date, end_date>` Project planning and task scheduling.
    -   **Sankey diagram** <br> `<from_node, to_node, value>` Depicting flows of resources or data.
    -   **Chord digram** <br> `<from_node, to_node, value>` Visualising relationships between data in a circular layout.

9.  Data visualistation helps create dashboards that consolidate reports and visualisations from multiple sources into one interface. Dashboards present a bird's eye view of the complete picture while also allowing you to drill down into the next level of information for each parameter, making them easy to understand and useful for generating reports quickly.

10. Commonly used visualisation tools in Python

    -   **Matplotlib** <br> Matplotlib is a comprehensive, low-level library for creating a wide variety of static, animated, and interactive visualisations in Python. It offers extensive control over plots and is widely used for its versatility and customisation capabilities, though it often requires more code for complex visualisations.
    -   **Seaborn** <br> Seaborn is a high-level library built on top of Matplotlib, designed for simpler use and better default aesthetics. It offers functions for statistical graphics and can create complex multivariate visualisations with less code, such as heatmaps and violin plots, providing a high-level interface for drawing attractive and informative statistical graphics.
    -   **Plotly** <br> Plotly is a high-level library designed for creating interactive plots, including multi-dimensional and 3D plots, and geographic maps. It features hover-over functionality to see point values and is known for producing publication-quality graphs suitable for web applications and dashboards.
    -   **Bokeh** <br> Bokeh is a powerful interactive visualisation library designed for modern web browsers. It excels in creating web-ready plots with high control over interactivity, making it ideal for building complex dashboards. Bokeh enables elegant and concise construction of versatile graphics and supports high-performance interactivity with large or streaming datasets.
    -   **Altair** <br> Altair is a declarative statistical visualisation library built on the grammar of graphics principles. It integrates well with Pandas and is known for creating beautiful, effective visualisations with concise syntax. Based on Vega and Vega-Lite, Altair is user-friendly, allowing for quick creation of complex visualisations by focusing on what you want to visualise rather than how to design it.
    -   **Plotnine** <br> Plotnine is a Python library based on the grammar of graphics, inspired by the ggplot2 package in R. It enables the creation of complex layered visualisations by breaking them down into semantic components like scales and layers. While it may seem less Pythonic compared to other libraries, it is highly powerful for those familiar with grammar of graphics concept.
    -   **Pygwalker** <br> A visualisation tool that allows users to create graphics interactively. It is often used for exploratory data analysis and can quickly generate insightful plots with minimal coding efforts.

    In conclusion, the best visualisation library for you depends on your specific needs and preferences. For exploratory data analysis in a Jupyter notebook, Matplotlib, Seaborn, or Pygwalker are excellent choices, with Pygwalker providing an interactive approach for quick, insightful visualisations. For interactive web applications, Plotly or Bokeh are more suitable. For complex layered visualisations, ggplot or Altair might be ideal. Matplotlib offers comprehensive control and customisation, Seaborn simplifies the creation of aesthetic statistical graphics, Plotly and Bokeh excel in interactive, web-ready visualisations, and Altair and Plotnine use the grammar of graphics for complex visualisations.

## Career Opportunities

1.  There are various job roles for data analysts, broadly classified into two categories:

    -   Data analyst specialist
    -   Domain specialist

2.  The data analyst specialist role focuses on developing technical and functional expertise. The career path for data analysts can vary depending on the industry and organisation size, but it generally involves starting as an associate or junior data analyst, progressing through data analyst, senior data analyst, lead data analyst, and principal data analyst roles. Skills advance from foundational to expert level in technical, statistical, and analytical areas, requiring familiarity with various tools, languages, and technologies.

    Communication, presentation, stakeholder management, and project management skills are also essential. Lead or principal data analysts may establish team processes, recommend software and tools, upskill team members, and expand the team. These responsibilities might overlap with managerial roles in some organisations.

3.  Domain specialists, also known as functional analysts, are experts in specific domains like healthcare, sales, finance, social media, or digital marketing. They may not be the most technically skilled but are recognised as authorities in their fields, holding titles such as marketing analyst, sales analyst, healthcare analyst, or social media analyst.

4.  Analytics-enabled job roles, like project managers, marketing managers, and HR managers, benefit from analytics skills to enhance efficiency and effectiveness. Many job openings in these roles are analytics-enabled as organisations increasingly rely on data for decision-making.

5.  Data analysts have opportunities to transition into other data professions by acquiring new skills. Potential career paths include:

    -   Data engineer
    -   Big data engineer
    -   Data scientist
    -   Business analyst
    -   Business intelligence analyst

    For example, a junior data analyst interested in data lakes and big data can become a big data engineer with further expertise. Similarly, those intrigued by the business aspect can transition into a business analyst or business intelligence analyst role. Many resources are available to support skill development and career advancement.

## Generative AI for Data Analysis

1.  The application of generative AI in data analysis is vast.

    -   **Data generation and imputation** <br> Generate synthetic datasets for testing and development. Generative AI can create synthetic datasets, solving the challenge of limited data availability. It creates synthetic datasets and augments existing ones, providing analysts with diverse and enriched data for robust analysis and model training. These models can fill in missing data, providing a more complete picture for analysts and data scientists.
    -   **Transforming data representation** <br> Tranforming data into different representations is another frontier. Generative AI can convert texts into images or vice versa, giving data analysts new perspectives and creative ways to represent complex information. Data preparation is crucial in data analytics journey. Generative AI contributes by automating and enhancing data cleaning, normalisation, transforming processes, and streamlining the path from raw data to actionable insights.
    -   **Querying databases** <br> When it comes to querying databases, generative AI brings efficiency and innovation. It can assist in formulating complex queries, optimising database interactions, and adapting to evolving data structures. Imagine having an intelligent assistant for your data queries. Generative AI empowers Q&A models, enabling users to interact with data naturally, ask questions in plain language, and receive meaningful insights in return.
    -   **Interactive visualisations and storytelling** <br> Generative AI can enhance the quality and aesthetics of data visualisations, making them informative and visually striking.
